import pickle
import psycopg2
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException, Header
from pydantic import BaseModel
import numpy as np
import cv2
# È˜terge sau comenteazÄƒ: import requests
from curl_cffi import requests # ðŸ‘ˆ Folosim requests din curl_cffi
from deepface import DeepFace
from concurrent.futures import ThreadPoolExecutor
from apscheduler.schedulers.background import BackgroundScheduler
import os
import pika
import json
import threading
import time

print(os.cpu_count())
os.environ["OPENCV_IO_ENABLE_OPENEXR"] = "0"
cv2.setNumThreads(0)
# --- CONFIGURARE DB ---
DB_HOST = os.getenv("DB_HOST", "localhost")
DB_NAME = os.getenv("DB_NAME", "fbi")
DB_USER = os.getenv("DB_USER", "keycloak")
DB_PASS = os.getenv("DB_PASS", "keycloak")
DB_PORT = os.getenv("DB_PORT", "5433")
API_KEY = os.getenv("API_KEY", "super_secret_face_recognition_api_key")
RUN_SYNC_AT_STARTUP = os.getenv("RUN_SYNC_AT_STARTUP", False)

DB_FILE = "fbi_vectors.pkl"
vector_db = []

# --- 1. LOGICA DE SINCRONIZARE ---
def fetch_urls_from_postgres():
    """Se conecteazÄƒ direct la Postgres È™i ia toate pozele."""
    try:
        print(f"ðŸ”Œ Connecting to DB at {DB_HOST}:{DB_PORT} as {DB_USER}...")
        conn = psycopg2.connect(
            host=DB_HOST,
            port=DB_PORT,      # <-- SpecificÄƒm portul aici
            database=DB_NAME,
            user=DB_USER,
            password=DB_PASS
        )
        cur = conn.cursor()
        # Presupunem cÄƒ tabela se numeÈ™te "WantedPersons" È™i coloana "ImageURL"
        # AtenÈ›ie la ghilimele, Postgres e sensibil la case dacÄƒ s-a creat cu EF Core
        sql_query = """
            SELECT wi."OriginalUrl", wi."LargeUrl"
            FROM "WantedImages" wi
            INNER JOIN "WantedPersons" wp ON wi."WantedPersonId" = wp."Id"
            WHERE wi."OriginalUrl" IS NOT NULL AND wi."OriginalUrl" != '' AND wi."LargeUrl" IS NOT NULL AND wi."LargeUrl" != ''
            LIMIT 200
        """

        cur.execute(sql_query)
        rows = cur.fetchall()

        cur.close()
        conn.close()
        # rows vine ca [('url1',), ('url2',)] -> le facem listÄƒ simplÄƒ
        return [ (row[0],row[1]) for row in rows if row[0]]
    except Exception as e:
        print(f"âŒ DB Connection Error: {e}")
        return []

def run_sync_job():
    """FuncÈ›ie optimizatÄƒ: Download Paralel -> Procesare SerialÄƒ"""
    global vector_db
    print("â° CRON JOB STARTED: Daily Vector Sync...")
    
    # 1. LuÄƒm URL-urile
    db_urls = fetch_urls_from_postgres()
    if not db_urls:
        print("âš ï¸ No URLs found.")
        return

    # 2. Vedem ce e nou
    # db_urls vine ca [(url_mic, url_mare), ...]
    # cached_urls È›ine minte doar url-ul final salvat
    cached_urls = {item["url"] for item in vector_db}
    
    # FiltrÄƒm tuplele. DacÄƒ url_mic SAU url_mare e deja Ã®n cache, sÄƒrim.
    new_entries = []
    for u_small, u_large in db_urls:
        if u_small not in cached_urls and (not u_large or u_large not in cached_urls):
            new_entries.append((u_small, u_large))
    
    if not new_entries:
        print("âœ… Nothing new to index.")
        return

    print(f"ðŸ”„ Indexing {len(new_entries)} new faces in batches...")

    # --- FUNCÈšIA DE DOWNLOAD (Doar download, fÄƒrÄƒ AI) ---
    def download_only(url_tuple):
        u_small, u_large = url_tuple
        
        # Helper intern
        def get_bytes(u):
            if not u: return None
            try:
                with requests.Session() as s:
                    resp = s.get(u, impersonate="chrome110", allow_redirects=True, timeout=10)
                    if resp.status_code != 200: return None
                    ct = resp.headers.get("content-type", "").lower()
                    if "text" in ct or "html" in ct: return None
                    return np.asarray(bytearray(resp.content), dtype=np.uint8)
            except:
                return None

        # ÃŽncercÄƒm mic
        img_arr = get_bytes(u_small)
        final_url = u_small

        # ÃŽncercÄƒm mare dacÄƒ mic a eÈ™uat
        if img_arr is None and u_large:
            img_arr = get_bytes(u_large)
            final_url = u_large
        
        if img_arr is None:
            return None

        # DecodÄƒm AICI (e safe Ã®n thread dacÄƒ cv2.setNumThreads(0) e setat)
        try:
            img = cv2.imdecode(img_arr, cv2.IMREAD_COLOR)
            return {"img": img, "url": final_url}
        except:
            return None

    # --- PROCESAREA ÃŽN LOTURI (BATCHES) ---
    # ProcesÄƒm cÃ¢te 10 ca sÄƒ nu umplem RAM-ul
    BATCH_SIZE = 10
    total_added = 0

    # Spargem lista Ã®n bucÄƒÈ›i de 10
    chunks = [new_entries[i:i + BATCH_SIZE] for i in range(0, len(new_entries), BATCH_SIZE)]

    for chunk in chunks:
        # PASUL 1: Download Paralel (Rapid)
        # Putem folosi mai mulÈ›i workeri aici cÄƒ e doar reÈ›ea, nu CPU
        downloaded_images = []
        with ThreadPoolExecutor(max_workers=5) as ex:
            results = ex.map(download_only, chunk)
            downloaded_images = [r for r in results if r is not None]

        # PASUL 2: AI Serial (Safe & Stable)
        # RulÄƒm DeepFace pe rÃ¢nd, pe thread-ul principal
        for item in downloaded_images:
            try:
                img_mat = item["img"]
                url_final = item["url"]
                
                print(f"ðŸ§  Analyzing AI: {url_final[:40]}...")
                
                objs = DeepFace.represent(
                    img_path=img_mat, 
                    model_name="Facenet", 
                    enforce_detection=False, 
                    detector_backend="opencv"
                )
                
                # SalvÄƒm Ã®n memorie
                vector_db.append({"url": url_final, "embedding": objs[0]["embedding"]})
                total_added += 1
                
            except Exception as e:
                print(f"âŒ AI Failed for {item['url']}: {e}")

        # SalvÄƒm pe disc dupÄƒ fiecare batch (ca sÄƒ nu pierdem progresul dacÄƒ picÄƒ curentul)
        with open(DB_FILE, 'wb') as f:
            pickle.dump(vector_db, f)
            
        print(f"ðŸ’¾ Saved batch. Total so far: {total_added}")

    print(f"ðŸ SYNC FINISHED. Added {total_added} new faces.")
# --- 2. CONFIGURARE SCHEDULER ---
scheduler = BackgroundScheduler()

@asynccontextmanager
async def lifespan(app: FastAPI):
    # -- STARTUP --
    # 1. ÃŽncÄƒrcÄƒm cache-ul existent
    global vector_db
    if os.path.exists(DB_FILE):
        try:
            with open(DB_FILE, 'rb') as f:
                vector_db = pickle.load(f)
            print(f"ðŸ“‚ Loaded {len(vector_db)} vectors from disk.")
        except:
            print("âš ï¸ Cache corrupt.")

    print("ðŸ° Starting RabbitMQ Consumer Thread...")
    worker_thread = threading.Thread(target=start_rabbitmq_consumer, daemon=True)
    worker_thread.start()

    # 2. Pornim Scheduler-ul
    # RuleazÄƒ Ã®n fiecare zi la ora 03:00 AM
    #scheduler.add_job(run_sync_job, 'cron', hour=3, minute=0)
    #scheduler.start()
    print("ðŸ•’ Scheduler started (Runs daily at 03:00).")

    
    
    # 3. (OpÈ›ional) RulÄƒm un sync rapid la start, ca sÄƒ nu aÈ™tepÈ›i pÃ¢nÄƒ mÃ¢ine
    if RUN_SYNC_AT_STARTUP:
        run_sync_job() 
    
    yield
    
    # -- SHUTDOWN --
    scheduler.shutdown()

# --- 3. INIT API ---
app = FastAPI(title="FBI Autonomous AI", lifespan=lifespan)

class SearchRequest(BaseModel):
    image_to_verify_url: str

# Helper matematic (acelaÈ™i ca Ã®nainte)
def find_cosine_distance(source, test):
    a = np.matmul(np.transpose(source), test)
    b = np.sum(np.multiply(source, source))
    c = np.sum(np.multiply(test, test))
    return 1 - (a / (np.sqrt(b) * np.sqrt(c)))

def load_image_from_url(url: str):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        success = False
        iterations = 5
        print("load_image_from_url : " + url)
        while iterations > 0 and success == False:
            response = requests.get(url, headers=headers, timeout=5, verify=False)
            if response.status_code == 200:
                print("success")
                image_array = np.asarray(bytearray(response.content), dtype=np.uint8)
                success = True
            iterations -= 1
        
        return  cv2.imdecode(image_array, cv2.IMREAD_COLOR) if success else None
    except Exception as e:
        print(str(e))
        return None

def get_embedding(img_data):
    try:
        # Facenet e rapid. Folosim detection 'opencv' pt vitezÄƒ
        objs = DeepFace.represent(img_path=img_data, model_name="Facenet", enforce_detection=False, detector_backend="opencv")
        return objs[0]["embedding"]
    except:
        return None

# Endpoint-ul de Search (RÄƒmÃ¢ne neschimbat, rapid)
@app.post("/fast-search")
async def fast_search(
        req: SearchRequest,
        x_api_key: str | None = Header(default=None, alias="X-FBI-Key")
    ):

    if not vector_db:
        raise HTTPException(status_code=400, detail="Database is empty. Call /sync-db first.")
    
    print(x_api_key)
    print(API_KEY)
    
    if x_api_key != API_KEY:
        raise HTTPException(status_code=400, detail="U must provide api key in ")


    target_img = load_image_from_url(req.image_to_verify_url)
    if target_img is None: raise HTTPException(status_code=400, detail="Download error")
    
    target_emb = get_embedding(target_img)
    if not target_emb: return {"matches": []}

    matches = []
    
    # ComparÄƒm cu toÈ›i vectorii din memorie
    for item in vector_db:
        dist = find_cosine_distance(target_emb, item["embedding"])
        
        # Pragul de 0.40 (poÈ›i sÄƒ-l scazi la 0.35 dacÄƒ vrei rezultate mai stricte)
        if dist < 0.40:
            matches.append({
                "url": item["url"],
                "confidence": round((1-dist)*100, 2)
            })
    
    # 1. SortÄƒm descrescÄƒtor dupÄƒ Ã®ncredere (cei mai buni primii)
    matches.sort(key=lambda x: x["confidence"], reverse=True)
    
    # 2. ðŸ”¥ AICI E MODIFICAREA: ReturnÄƒm doar primii 5
    top_5_matches = matches[:5] 

    return {"matches": top_5_matches}

@app.get("/list-faces")
def list_faces():
    """ReturneazÄƒ lista celor 11 suspecÈ›i indexaÈ›i"""
    return [item["url"] for item in vector_db]

# --- LOGICA RABBITMQ WORKER ---
def process_rabbitmq_message(ch, method, properties, body):
    """Callback-ul care se executÄƒ cÃ¢nd vine un mesaj Ã®n coadÄƒ"""
    try:
        # 1. DespachetÄƒm mesajul
        message = json.loads(body)["message"]
        print(f"ðŸ° [RabbitMQ] Received job: {message}")
        
        report_id = message.get("ReportId") or message.get("reportId") # Handle both cases
        image_url = message.get("ImageUrl") or message.get("imageUrl")
        
        if not report_id or not image_url:
            print("âš ï¸ Invalid message format.")
            ch.basic_ack(delivery_tag=method.delivery_tag)
            return

        # 2. Logica de AI (CopiatÄƒ/AdaptatÄƒ din fast_search)
        target_img = load_image_from_url(image_url)
        if target_img is None:
            print(f"âŒ Failed to download image: {image_url}")
            # Putem marca raportul ca Failed Ã®n DB aici dacÄƒ vrei
            ch.basic_ack(delivery_tag=method.delivery_tag)
            return

        target_emb = get_embedding(target_img)
        
        matches = []
        if target_emb:
            # ComparÄƒm cu baza vectoriala din memorie
            for item in vector_db:
                dist = find_cosine_distance(target_emb, item["embedding"])
                if dist < 0.40:
                    matches.append({
                        "url": item["url"],
                        "confidence": float(round((1-dist)*100, 2))
                    })
            matches.sort(key=lambda x: x["confidence"], reverse=True)
            top_5_matches = matches[:5]
        else:
            top_5_matches = []

        # 3. Scriem rezultatele Ã®n DB (Postgres)
        save_results_to_postgres(report_id, top_5_matches)
        
        # 4. ConfirmÄƒm mesajul (ACK) ca sÄƒ fie È™ters din coadÄƒ
        print(f"âœ… Job {report_id} finished. Matches found: {len(top_5_matches)}")

        response_message = {
            "reportId": report_id,
            "success": True
        }
        
        publish_queue = os.getenv("RABBIT_QUEUE_PUBLISH", "analysis-finished-queue")
        ch.queue_declare(queue=publish_queue, durable=True)
        
        ch.basic_publish(
            exchange='',
            routing_key='analysis-finished-queue',
            body=json.dumps(response_message),
            properties=pika.BasicProperties(
                delivery_mode=2,  # Mesaj persistent
                content_type='application/json'
            )
        )

        print(f"âœ… Job {report_id} finished & notification sent.")
        ch.basic_ack(delivery_tag=method.delivery_tag)

        ch.basic_ack(delivery_tag=method.delivery_tag)

    except Exception as e:
        print(f"âŒ Worker Error: {e}")
        # ÃŽn caz de eroare gravÄƒ, dÄƒm NACK (mesajul revine Ã®n coadÄƒ sau merge Ã®n Dead Letter)
        # ch.basic_nack(delivery_tag=method.delivery_tag, requeue=False)
        ch.basic_ack(delivery_tag=method.delivery_tag) # DÄƒm ACK ca sÄƒ nu blocheze coada la infinit


def save_results_to_postgres(report_id, matches):
    """FuncÈ›ie auxiliarÄƒ pentru INSERT"""
    try:
        conn = psycopg2.connect(
            host=DB_HOST, port=DB_PORT, database=DB_NAME, user=DB_USER, password=DB_PASS
        )
        cur = conn.cursor()
        
        # 1. Insert Match-uri
        if matches:
            args_str = ','.join(cur.mogrify("(%s, %s, %s)", (report_id, m['url'], m['confidence'])).decode('utf-8') for m in matches)
            cur.execute("INSERT INTO \"PersonMatchResults\" (\"LocationWantedPersonId\", \"ImageUrl\", \"Confidence\") VALUES " + args_str)
        
        # 2. Update Status Raport (Status = 1 aka Completed)
        # AsigurÄƒ-te cÄƒ ID-ul statusului '1' corespunde cu Enum-ul din C# (Completed)
        cur.execute('UPDATE "LocationWantedPersons" SET "Status" = 1 WHERE "Id" = %s', (report_id,))
        
        conn.commit()
        cur.close()
        conn.close()
    except Exception as e:
        print(f"âŒ DB Save Error: {e}")

def start_rabbitmq_consumer():
    """AceastÄƒ funcÈ›ie ruleazÄƒ Ã®n thread separat"""
    while True:
        try:
            print("ðŸ° Connecting to RabbitMQ...")
            # 'rabbitmq' este numele containerului din docker-compose
            # DacÄƒ rulezi local python (fÄƒrÄƒ docker), pune 'localhost'
            rabbit_host = os.getenv("RABBIT_HOST", "localhost") 
            rabbit_user = os.getenv("RABBIT_USER", "guest")
            rabbit_pass = os.getenv("RABBIT_PASS", "guest")

            
            connection = pika.BlockingConnection(
                pika.ConnectionParameters(
                    host=rabbit_host, 
                    credentials=pika.PlainCredentials( rabbit_user, rabbit_pass),
                    heartbeat=600 # Keep-alive mai lung
                )
            )
            channel = connection.channel()
            
            # Ne asigurÄƒm cÄƒ existÄƒ coada (Idempotent)
            # IMPORTANT: Numele trebuie sÄƒ fie identic cu cel din C# (MassTransit)
            # MassTransit de obicei foloseÈ™te numele complet al contractului, ex: "FbiApi.Contracts:AnalyzeFaceCommand"
            # Sau dacÄƒ ai configurat un endpoint specific Ã®n C#, foloseÈ™te numele Äƒla.
            # Pentru simplitate, sÄƒ zicem cÄƒ ai configurat Ã®n C# queue name = 'face-analysis-queue'
            QUEUE_NAME = os.getenv("RABBIT_QUEUE", 'face-analysis-queue') 
            
            channel.queue_declare(queue=QUEUE_NAME, durable=True)
            channel.basic_qos(prefetch_count=1) # Ia cÃ¢te un mesaj pe rÃ¢nd
            
            channel.basic_consume(queue=QUEUE_NAME, on_message_callback=process_rabbitmq_message)
            
            print("ðŸ° Consumer started. Waiting for messages...")
            channel.start_consuming()
        
        except Exception as e:
            print(f"âš ï¸ RabbitMQ Connection Failed: {e}. Retrying in 5s...")
            time.sleep(5)